{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading CSV Files from Azure Blob Storage into Snowflake using Snowpark\n",
        "\n",
        "In this notebook, we will demonstrate how to load a CSV file from an Azure Blob Storage into Snowflake using Snowpark for Python. We will:\n",
        "\n",
        "1. Establish a Snowflake session using Snowpark.\n",
        "2. Create an external stage in Snowflake that references the Azure Blob Storage location.\n",
        "3. Load the CSV data from the external stage into a Snowpark DataFrame.\n",
        "4. Perform basic data operations using the Snowpark DataFrame API.\n",
        "5. Write the DataFrame to a Snowflake table and query it using SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import col\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Establish a Snowflake Session\n",
        "\n",
        "First, we need to establish a connection to Snowflake using Snowpark. Replace the placeholder values with your Snowflake account details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define Snowflake connection parameters\n",
        "connection_parameters = {\n",
        "    \"account\": \"<your_account_identifier>\",\n",
        "    \"user\": \"<your_username>\",\n",
        "    \"password\": \"<your_password>\",\n",
        "    \"role\": \"<your_user_role>\",\n",
        "    \"warehouse\": \"<your_warehouse>\",\n",
        "    \"database\": \"<your_database>\",\n",
        "    \"schema\": \"<your_schema>\"\n",
        "}\n",
        "\n",
        "# Create a Snowflake session\n",
        "session = Session.builder.configs(connection_parameters).create()\n",
        "\n",
        "# Add a query tag to the session for troubleshooting and performance monitoring\n",
        "session.query_tag = json.dumps({\n",
        "    \"origin\": \"sf_sit-is\",\n",
        "    \"name\": \"notebook_demo_pack\",\n",
        "    \"version\": {\"major\": 1, \"minor\": 0},\n",
        "    \"attributes\": {\"is_quickstart\": 1, \"source\": \"notebook\", \"vignette\": \"csv_from_azure_blob\"}\n",
        "})\n",
        "\n",
        "print(session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create an External Stage in Snowflake\n",
        "\n",
        "Next, we will create an external stage in Snowflake that references the Azure Blob Storage container where our CSV file is stored. Replace the placeholder values with your Azure Blob Storage details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "sql"
      },
      "source": [
        "CREATE OR REPLACE STAGE azure_stage\n",
        "  URL = 'azure://<your_storage_account_name>.blob.core.windows.net/<your_container_name>'\n",
        "  CREDENTIALS = (AZURE_SAS_TOKEN = '<your_sas_token>');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: List Files in the External Stage\n",
        "\n",
        "Let's list the files available in the external stage to verify our CSV file is accessible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "sql"
      },
      "source": [
        "LIST @azure_stage;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load CSV Data into a Snowpark DataFrame\n",
        "\n",
        "We can now load the CSV file from the external stage into a Snowpark DataFrame. By setting `infer_schema=True`, Snowflake will automatically infer the schema based on the data types present in the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data from the CSV file into a Snowpark DataFrame\n",
        "df = session.read.options({\"infer_schema\": True}).csv('@azure_stage/<your_csv_file_name>.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Perform Data Operations Using Snowpark DataFrame API\n",
        "\n",
        "Now that the data is loaded into a Snowpark DataFrame, we can perform various data operations using the Snowpark DataFrame API. For example, we can compute descriptive statistics on the columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute descriptive statistics on the DataFrame\n",
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Write the DataFrame to a Snowflake Table\n",
        "\n",
        "We can write the DataFrame into a Snowflake table called `APP_ORDER` and query it using SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write the DataFrame to a Snowflake table\n",
        "df.write.mode(\"overwrite\").save_as_table(\"APP_ORDER\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "sql"
      },
      "source": [
        "-- Preview the newly created APP_ORDER table\n",
        "SELECT * FROM APP_ORDER;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Read the Table Back into a Snowpark DataFrame\n",
        "\n",
        "Finally, we can read the table back into a Snowpark DataFrame using the `session.table` method and continue to query and process the data."

::contentReference[oaicite:0]{index=0}
 
